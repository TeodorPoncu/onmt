model_dir: id_transformer/

data:
  source_tokenization: config/tokenization/aggressive.yml
  source_words_vocabulary: data/id_transformer/src-vocab.txt
  target_words_vocabulary: data/id_transformer/tgt-vocab.txt
  train_features_file: data/id_transformer/src-train.txt
  train_labels_file: data/id_transformer/tgt-train.txt
  eval_features_file: data/id_transformer/src-val.txt
  eval_labels_file: data/id_transformer/tgt-val.txt

train:
  batch_type: tokens
  batch_size: 512
  bucket_width: 1
  sample_buffer_size: 500000
  save_summary_steps: 100

params:
  average_loss_in_time: True
  optimizer: LazyAdamOptimizer
  optimizer_params: 
    beta1: 0.9
    beta2: 0.998
  learning_rate: 1.0 # The scale constant.
  clip_gradients: null
  decay_step_duration: 8 # 1 decay step is 8 training steps.
  decay_type: noam_decay_v2
  decay_params:
    model_dim: 128
    warmup_steps: 2000 # (= 16000 training steps).
  start_decay_steps: 0

  # (optional) Model exporter(s) to use during the training and evaluation loop:
  # last, final, best, or null (default: last).
  exporters: last

score:
  # (optional) The batch size to use (default: 64).
  batch_size: 64
  # (optional) The number of threads to use for processing data in parallel (default: 1).
  num_threads: 1
  # (optional) The number of batches to prefetch asynchronously (default: 1).
  prefetch_buffer_size: 1

  # (optional) Also report token-level cross entropy.
  with_token_level: true
  # (optional) Also output the alignments (can be: "null", "hard", default: "null").
  with_alignments: null