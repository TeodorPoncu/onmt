model_dir: intermediate_parsing_transformer/

data:
  source_tokenization: config/tokenization/aggressive.yml
  target_tokenization: config/tokenization/aggressive.yml
  source_vocabulary_1: data/intermediate_parsing_transformer/tbl-vocab.txt
  source_vocabulary_2: data/intermediate_parsing_transformer/col-vocab.txt
  source_vocabulary_3: data/intermediate_parsing_transformer/src-vocab.txt
  source_vocabulary_4: data/intermediate_parsing_transformer/skt-vocab.txt
  target_vocabulary: data/intermediate_parsing_transformer/tgt-vocab.txt
  train_features_file:
    - data/intermediate_parsing_transformer/tbl-train.txt
    - data/intermediate_parsing_transformer/col-train.txt
    - data/intermediate_parsing_transformer/src-train.txt
    - data/intermediate_parsing_transformer/skt-train.txt
  train_labels_file: data/intermediate_parsing_transformer/tgt-train.txt
  eval_features_file:
    - data/intermediate_parsing_transformer/tbl-val.txt
    - data/intermediate_parsing_transformer/col-val.txt
    - data/intermediate_parsing_transformer/src-train.txt
    - data/intermediate_parsing_transformer/skt-train.txt
  eval_labels_file: data/intermediate_parsing_transformer/tgt-val.txt

train:
  batch_type: tokens
  batch_size: 1024
  steps: 56000
  bucket_width: 1
  sample_buffer_size: 500000
  save_summary_steps: 100

params:
  average_loss_in_time: True
  optimizer: LazyAdamOptimizer
  optimizer_params: 
    beta1: 0.9
    beta2: 0.998
  learning_rate: 1.0 # The scale constant.
  clip_gradients: null
  decay_step_duration: 8 # 1 decay step is 8 training steps.
  decay_type: noam_decay_v2
  decay_params:
    model_dim: 512
    warmup_steps: 2000 # (= 16000 training steps).
  start_decay_steps: 0

beam_width: 5
length_penalty: 0.2
coverage_penalty: 0.2
replace_unknown_target: true




eval:
  # (optional) The batch size to use (default: 32).
  batch_size: 32
  # (optional) The number of threads to use for processing data in parallel (default: 1).
  num_threads: 1
  # (optional) The number of batches to prefetch asynchronously (default: 1).
  prefetch_buffer_size: 1
  # (optional) Evaluate every this many seconds (default: 18000).
  eval_delay: 600
  # (optional) Save evaluation predictions in model_dir/eval/.
  save_eval_predictions: true
  # (optional) Evalutator or list of evaluators that are called on the saved evaluation predictions.
  # Available evaluators: sacreBLEU, BLEU, BLEU-detok, ROUGE
  external_evaluators: 
    - sacreBLEU
    - BLEU
    - BLEU-detok


  # (optional) Model exporter(s) to use during the training and evaluation loop:
  # last, final, best, or null (default: last).
  exporters: best

score:
  # (optional) The batch size to use (default: 64).
  batch_size: 64
  # (optional) The number of threads to use for processing data in parallel (default: 1).
  num_threads: 1
  # (optional) The number of batches to prefetch asynchronously (default: 1).
  prefetch_buffer_size: 1

  # (optional) Also report token-level cross entropy.
  with_token_level: true
  # (optional) Also output the alignments (can be: "null", "hard", default: "null").
  with_alignments: null

infer:
  batch_size: 1